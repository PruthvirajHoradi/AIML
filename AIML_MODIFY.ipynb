{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIML MODIFY.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TW 1 A*\n"
      ],
      "metadata": {
        "id": "aeRytRXkFH9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Describe the graph \n",
        "Graph_nodes = {\n",
        "                'A': [('B', 2), ('E', 3)],\n",
        "                'B': [('C', 1),('G', 9)],\n",
        "                'C': [],\n",
        "                'E': [('D', 6)],\n",
        "                'D': [('G', 1)],\n",
        "                'G': []\n",
        "              }\n",
        "\n",
        "#Heuristic Distance\n",
        "H_dist = {\n",
        "            'A': 11,\n",
        "            'B': 6,\n",
        "            'C': 99,\n",
        "            'D': 1,\n",
        "            'E': 7,\n",
        "            'G': 0,\n",
        "             \n",
        "        }\n",
        "\n",
        "g = {} #stores distance from starting node\n",
        "\n",
        "parents = {} # Contains parent node\n",
        "\n",
        "def f(n):\n",
        "    return g[n] + H_dist[n]\n",
        "\n",
        "\n",
        "def aStarAlgo(start_node, stop_node):\n",
        "         \n",
        "        open_set = set(start_node) \n",
        "        closed_set = set()\n",
        " \n",
        "\n",
        "        g[start_node] = 0\n",
        "\n",
        "     \n",
        "        parents[start_node] = start_node\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "        while len(open_set) > 0:\n",
        "\n",
        "            n = None\n",
        "\n",
        "            #-----------------------------------------------------\n",
        "           \n",
        "            for v in open_set:\n",
        "                if n == None or f(v) < f(n):\n",
        "                    n = v\n",
        "\n",
        "            #-------------------------------------------------------\n",
        "           \n",
        "            if n == stop_node:\n",
        "                path = []\n",
        " \n",
        "                while parents[n] != n:\n",
        "                    path.append(n)\n",
        "                    n = parents[n]\n",
        " \n",
        "                path.append(start_node)\n",
        " \n",
        "                path.reverse()        \n",
        "                print('Path found: {}'.format(path))\n",
        "                return path\n",
        "\n",
        "            #---------------------------------------------------\n",
        "            if Graph_nodes[n] == None:\n",
        "                pass\n",
        "            else:\n",
        "                for (child, dist) in Graph_nodes[n]:\n",
        "\n",
        "                   \n",
        "                    if child not in open_set and child not in closed_set:\n",
        "                        open_set.add(child)\n",
        "                        parents[child] = n\n",
        "                        g[child] = g[n] + dist\n",
        "                         \n",
        "                    #--------------------------------------------------\n",
        "                   \n",
        "                    elif g[child] > g[n] + dist:\n",
        "                            g[child] = g[n] + dist \n",
        "                            parents[child] = n\n",
        "                        #--------------------------------------------    \n",
        "                           \n",
        "                            if child in closed_set:\n",
        "                                closed_set.remove(child)\n",
        "                                open_set.add(child)\n",
        "\n",
        "\n",
        "                #--------------------------------------------------------\n",
        "                                                    \n",
        "                open_set.remove(n)\n",
        "                closed_set.add(n)\n",
        "\n",
        "\n",
        "        #if everything fails\n",
        "        print('Path does not exist!')\n",
        "        return None\n",
        "\n",
        "aStarAlgo('A', 'G')   "
      ],
      "metadata": {
        "id": "Jnl_VAAxFKbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 2 BFS"
      ],
      "metadata": {
        "id": "qO0aEHW-Fc3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = {\n",
        "\n",
        "    5 : [3 , 7],\n",
        "    3 : [2 , 4],\n",
        "    7 : [8],\n",
        "    4 : [8],\n",
        "    2 : [],\n",
        "    8 : [],\n",
        "}\n",
        "\n",
        "visited = [] #List to keep track of visited nodes\n",
        "queue = [] #List of child nods which are not visited\n",
        "\n",
        "def BFS(graph,root):\n",
        "\n",
        "    visited.append(root)\n",
        "    queue.append(root) #root is added because it's children are not visited yet\n",
        "\n",
        "    while queue:\n",
        "\n",
        "        s = queue.pop(0)\n",
        "        print(s, end = \" \")\n",
        "\n",
        "        for child in graph[s]:\n",
        "            if child not in visited:\n",
        "                visited.append(child)\n",
        "                queue.append(child)\n",
        "\n",
        "BFS(graph,5)"
      ],
      "metadata": {
        "id": "rqHcFT8lFfC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 3 decision tree\n",
        "\n"
      ],
      "metadata": {
        "id": "L05ascp8EXp7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-fwdYf4EOqq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "def loadDataSet():\n",
        "  Ir =load_iris()\n",
        "  return Ir\n",
        "\n",
        "def trainModel(Ir):\n",
        "  clf = tree.DecisionTreeClassifier()\n",
        "  clf = clf.fit(Ir.data, Ir.target)\n",
        "  return clf\n",
        "\n",
        "def display(clf, Ir):\n",
        "  plt.figure(figsize = (40,40))\n",
        "  tree.plot_tree(clf, filled = True)\n",
        "  plt.show()\n",
        "\n",
        "Ir1 = loadDataSet()\n",
        "\n",
        "#print (Ir1.feature_names)\n",
        "#print (Ir1.target_names)\n",
        "\n",
        "\n",
        "M = trainModel(Ir1)\n",
        "\n",
        "display(M,Ir1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 3 (1) car_evaluation"
      ],
      "metadata": {
        "id": "YB-uqJstEfwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import tree\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "balance_data=pd.read_csv(\"car_evaluation.csv\")\n",
        "#print \"Dataset:: \"\n",
        "\n",
        "#df1.head()\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "balance_data = balance_data.apply(le.fit_transform)\n",
        "\n",
        "X = balance_data.values[:, 0:5]\n",
        "Y = balance_data.values[:,6]\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 100)\n",
        "clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
        "                               max_depth=3, min_samples_leaf=5)\n",
        "\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "def display_image(clf_gini, X,Y):\n",
        "    plt.figure()\n",
        "    plt.figure(figsize=(40, 40))\n",
        "    tree.plot_tree(clf_gini,filled=True)\n",
        "    plt.show()\n",
        "\n",
        "def train_model(X,Y):\n",
        "  clf = tree.DecisionTreeClassifier()\n",
        "  clf = clf.fit(X, Y)\n",
        "  return clf\n",
        "\n",
        "decision_tree_classifier = train_model(X , Y)\n",
        "display_image(decision_tree_classifier, X,Y )\n"
      ],
      "metadata": {
        "id": "Rvw4CmXjElCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 3 (2) BankNote_Authentication"
      ],
      "metadata": {
        "id": "8T4C0-nWEofx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "def loadDataSet():\n",
        "  Ir =pd.read_csv(\"BankNote_Authentication.csv\")\n",
        "  return Ir\n",
        "\n",
        "def trainModel(Ir):\n",
        "  clf = tree.DecisionTreeClassifier()\n",
        "  clf = clf.fit(Ir.drop('class',axis = 'columns'), Ir['class'])\n",
        "  return clf\n",
        "\n",
        "def display(clf, Ir):\n",
        "  plt.figure(figsize = (40,40))\n",
        "  tree.plot_tree(clf, filled = True)\n",
        "  plt.show()\n",
        "\n",
        "Ir1 = loadDataSet()\n",
        "\n",
        "#print (Ir1.feature_names)\n",
        "#print (Ir1.target_names)\n",
        "\n",
        "\n",
        "M = trainModel(Ir1)\n",
        "\n",
        "display(M,Ir1)\n",
        "\n"
      ],
      "metadata": {
        "id": "oVKO33KoEtck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 3 (3) Petrol consumption"
      ],
      "metadata": {
        "id": "Vf0zcbHtEvUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "def loadDataSet():\n",
        "  Ir =pd.read_csv(\"petrol_consumption.csv\")\n",
        "  return Ir\n",
        "\n",
        "def trainModel(Ir):\n",
        "  clf = tree.DecisionTreeClassifier()\n",
        "  clf = clf.fit(Ir.drop('Petrol_Consumption',axis = 'columns'), Ir['Petrol_Consumption'])\n",
        "  return clf\n",
        "\n",
        "def display(clf, Ir):\n",
        "  plt.figure(figsize = (40,40))\n",
        "  tree.plot_tree(clf, filled = True)\n",
        "  plt.show()\n",
        "\n",
        "Ir1 = loadDataSet()\n",
        "\n",
        "#print (Ir1.feature_names)\n",
        "#print (Ir1.target_names)\n",
        "\n",
        "\n",
        "M = trainModel(Ir1)\n",
        "\n",
        "display(M,Ir1)\n",
        "\n"
      ],
      "metadata": {
        "id": "EZ6N69K3Eykp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 4 naive bayes\n"
      ],
      "metadata": {
        "id": "HIU0ojzGE2DZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "\n",
        "X = wine.data\n",
        "Y = wine.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.03,random_state=109)\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train,Y_train)\n",
        "\n",
        "Y_pred = gnb.predict(X_test)\n",
        "print(Y_pred)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(\"Accuracy\",metrics.accuracy_score(Y_test,Y_pred))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = np.array(confusion_matrix(Y_test,Y_pred))\n",
        "print(cm)\n",
        "\n",
        "print(classification_report(Y_test,Y_pred))"
      ],
      "metadata": {
        "id": "a4DH6E1eE6qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 4 (1) English text\n"
      ],
      "metadata": {
        "id": "SAswVe2gFuPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.datasets as skd\n",
        "\n",
        "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "news_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "news_test = fetch_20newsgroups(subset='train', categories=categories)\n",
        "\n",
        "print(news_train.keys())\n",
        "\n",
        "print(news_train['target_names'])\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vect=CountVectorizer()\n",
        "\n",
        "X_tarin_tf=count_vect.fit_transform(news_train.data)\n",
        "\n",
        "print(X_tarin_tf)\n",
        "\n",
        "X_tarin_tf.shape\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer()\n",
        "\n",
        "X_train_tfidf=tfidf_transformer.fit_transform(X_tarin_tf)\n",
        "\n",
        "X_train_tfidf.shape\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf= MultinomialNB().fit(X_train_tfidf, news_train.target)\n",
        "\n",
        "X_test_tf=count_vect.transform(news_test.data)\n",
        "\n",
        "X_test_tfidf=tfidf_transformer.transform(X_test_tf)\n",
        "\n",
        "predicted=clf.predict(X_test_tfidf)\n",
        "\n",
        "predicted\n",
        "\n",
        "from sklearn import metrics \n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Accuracy\",accuracy_score(news_test.target,predicted))\n",
        "\n",
        "print(metrics.confusion_matrix(news_test.target,predicted))"
      ],
      "metadata": {
        "id": "9YElpkEfGGGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 4 (2) E-mail"
      ],
      "metadata": {
        "id": "QwaeQK3zGcyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "dataset = pd.read_csv('spamham.csv')\n",
        "dataset.head() \n",
        "dataset .columns #Index(['text', 'spam'], dtype='object')\n",
        "dataset.shape  #(5728, 2)\n",
        "#Checking for duplicates and removing them\n",
        "dataset.drop_duplicates(inplace = True)\n",
        "dataset.shape  #(5695, 2)\n",
        "#Checking for any null entries in the dataset\n",
        "print (pd.DataFrame(dataset.isnull().sum()))\n",
        "#Checking class distribution\n",
        "dataset.groupby('spam').count()\n",
        "'''\n",
        "spam      \n",
        "0     4327\n",
        "1     1368\n",
        "'''\n",
        "dataset['length'] = dataset['text'].map(lambda text: len(text))\n",
        "#Let's plot histogram for length distribution by spam\n",
        "dataset.hist(column='length', by='spam', bins=50)\n",
        "#we can see some extreme outliers, we'll set a threshold for text length and plot the histogram again\n",
        "dataset[dataset.length < 10000].hist(column='length', by='spam', bins=100)\n",
        "#Using Natural Language Processing to cleaning the text to make one corpus\n",
        "# Cleaning the texts\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#Every mail starts with 'Subject :' lets remove this from each mail \n",
        "dataset['text']=dataset['text'].map(lambda text: text[1:])\n",
        "dataset['text'] = dataset['text'].map(lambda text:re.sub('[^A-Za-z0-9]+', ' ',text)).apply(lambda x: (x.lower()).split())\n",
        "ps = PorterStemmer()\n",
        "corpus=dataset['text'].apply(lambda text_list:' '.join(list(map(lambda word:ps.stem(word),(list(filter(lambda text:text not in set(stopwords.words('english')),text_list)))))))\n",
        "\n",
        "'''\n",
        "# Implemenation of corpus using function\n",
        "corpus=[]\n",
        "def fun(i):\n",
        "    #return (list(filter(lambda text:text not in set(stopwords.words('english')),i)))\n",
        "    return list(map(lambda word:ps.stem(word),(list(filter(lambda text:text not in set(stopwords.words('english')),i)))))\n",
        "corpus= dataset['text'][0:5].apply(lambda i: fun(i))\n",
        "'''\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus.values).toarray()\n",
        "y = dataset.iloc[:, 1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "\n",
        "# Fitting classifier to the Training set\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
        "classifier.fit(X_train , y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "#this function computes subset accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, y_pred) #0.99122036874451269\n",
        "accuracy_score(y_test, y_pred,normalize=False) #1129 out of 1139\n",
        "\n",
        "# Applying k-Fold Cross Validation\n",
        "from sklearn.cross_validation import cross_val_score\n",
        "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)#array([ 0.98903509,  0.98903509,  0.99122807,  0.98026316,  0.98245614,0.98903509,  0.98901099,  0.99340659,  0.99340659,  0.98681319])\n",
        "accuracies.mean()# 0.98836899942163114\n",
        "accuracies.std()#0.0040467182445280397\n",
        "\n",
        "\n",
        "# Create CV training and test scores for various training set sizes\n",
        "from sklearn.learning_curve import learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(classifier, X, y,cv=10, scoring='accuracy', n_jobs=-1, train_sizes=np.linspace(0.01, 1.0, 10))\n",
        "\n",
        "# Create means and standard deviations of training set scores\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "\n",
        "# Create means and standard deviations of test set scores\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "# Draw lines\n",
        "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
        "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
        "\n",
        "# Create plot\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0iqSitmxGlGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 4 (3) balance dataset"
      ],
      "metadata": {
        "id": "KY-7xGpOGu3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "\n",
        "# Function importing Dataset\n",
        "def importdata():\n",
        "    balance_data = pd.read_csv('balance-scale.csv',sep= ',', header = None)\n",
        "    # Printing the dataswet shape\n",
        "    print (\"Dataset Length: \", len(balance_data))\n",
        "    print (\"Dataset Shape: \", balance_data.shape)\n",
        "    # Printing the dataset obseravtions\n",
        "    print (\"Dataset: \",balance_data.head())\n",
        "    return balance_data\n",
        "\n",
        "# Function to split the dataset\n",
        "def splitdataset(balance_data):\n",
        "\n",
        "\t# Separating the target variable\n",
        "\tX = balance_data.values[:, 1:5]\n",
        "\tY = balance_data.values[:, 0]\n",
        "\n",
        "\t# Splitting the dataset into train and test\n",
        "\tX_train, X_test, y_train, y_test = train_test_split(\n",
        "\tX, Y, test_size = 0.3, random_state = 100)\n",
        "\t\n",
        "\treturn X, Y, X_train, X_test, y_train, y_test\n",
        "\t\n",
        "\n",
        "\t\n",
        "# Function to perform training with entropy.\n",
        "def train(X_train, X_test, y_train):\n",
        "    #Create a Gaussian Classifier\n",
        "\n",
        "    gnb = GaussianNB()\n",
        "    gnb.fit(X_train, y_train)\n",
        "    return gnb\n",
        "\n",
        "\n",
        "# Function to make predictions\n",
        "def prediction(X_test, clf_object):\n",
        "\n",
        "\t# Predicton on test with giniIndex\n",
        "\ty_pred = clf_object.predict(X_test)\n",
        "\tprint(\"Predicted values:\")\n",
        "\tprint(y_pred)\n",
        "\treturn y_pred\n",
        "\t\n",
        "# Function to calculate accuracy\n",
        "def cal_accuracy(y_test, y_pred):\n",
        "\t\n",
        "\tprint(\"Confusion Matrix: \",\n",
        "\t\tconfusion_matrix(y_test, y_pred))\n",
        "\t\n",
        "\tprint (\"Accuracy : \",\n",
        "\taccuracy_score(y_test,y_pred)*100)\n",
        "\t\n",
        "\tprint(\"Report : \",\n",
        "\tclassification_report(y_test, y_pred))\n",
        "\n",
        "    \n",
        "data = importdata()\n",
        "X, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n",
        "clf = train(X_train, X_test, y_train)\n",
        "\t\n",
        "\n",
        "print(\"Results Using Entropy:\")\n",
        "# Prediction using entropy\n",
        "y_pred_entropy = prediction(X_test, clf)\n",
        "cal_accuracy(y_test, y_pred_entropy)\n",
        "\t"
      ],
      "metadata": {
        "id": "Ma9IsV42G58I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 5 knn"
      ],
      "metadata": {
        "id": "n23CEPMRHC0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data\n",
        "Y = iris.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33,random_state=109)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train,Y_train)\n",
        "Y_pred = knn.predict(X_test)\n",
        "print(Y_pred)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Correct prediction\",accuracy_score(Y_test,Y_pred))\n",
        "print(\"Wrong prediction\",(1-accuracy_score(Y_test,Y_pred)))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "cm = confusion_matrix(Y_test,Y_pred)\n",
        "print(cm)\n",
        "\n",
        "print(classification_report(Y_test,Y_pred))\n",
        "\n",
        "plt.bar(\"Correct prediction\",accuracy_score(Y_test,Y_pred))\n",
        "plt.bar(\"Wrong prediction\",(1-accuracy_score(Y_test,Y_pred)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "itKdZCIvHGfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 5 (1) Load digit"
      ],
      "metadata": {
        "id": "cR6QySXIHJIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "digit = datasets.load_digits()\n",
        "print(\"Digit dataset loaded...\")\n",
        "\n",
        "print(digit.keys())\n",
        "for i in range(len(digit.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(digit.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(digit.data, digit.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "correct_pred = accuracy_score(y_test,y_pred)*100\n",
        "wrong_pred = (1-accuracy_score(y_test,y_pred))*100\n",
        "print(\"Correct predicition results\",correct_pred,\"%\")\n",
        "print(\"Worng predicition results\",wrong_pred,\"%\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.bar('Correct predicition', correct_pred, color ='green')\n",
        "plt.bar('Wrong predicition', wrong_pred, color ='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S4yEmaQiHNFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 5 (2) Load wine"
      ],
      "metadata": {
        "id": "NzXlgWKOHTbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "wine = datasets.load_wine()\n",
        "print(\"Wine dataset loaded...\")\n",
        "\n",
        "print(wine.keys())\n",
        "for i in range(len(wine.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(wine.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "correct_pred = accuracy_score(y_test,y_pred)*100\n",
        "wrong_pred = (1-accuracy_score(y_test,y_pred))*100\n",
        "print(\"Correct predicition results\",correct_pred,\"%\")\n",
        "print(\"Worng predicition results\",wrong_pred,\"%\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.bar('Correct predicition', correct_pred, color ='green')\n",
        "plt.bar('Wrong predicition', wrong_pred, color ='red')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aFR1W7rjHW0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 6 (1)"
      ],
      "metadata": {
        "id": "ACNCdgHKHZ3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris dataset loaded...\")\n",
        "\n",
        "print(iris.keys())\n",
        "\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(iris.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3, p = 2 )\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"Correct predicition results\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "print(\"Worng predicition results\",(1-accuracy_score(y_test,y_pred))*100,\"%\")"
      ],
      "metadata": {
        "id": "LW8TeHdUHjwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 6 (2)"
      ],
      "metadata": {
        "id": "B0wJxYwlHk0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris dataset loaded...\")\n",
        "\n",
        "print(iris.keys())\n",
        "\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(iris.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3,metric = 'hamming' )\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"Correct predicition results\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "print(\"Worng predicition results\",(1-accuracy_score(y_test,y_pred))*100,\"%\")"
      ],
      "metadata": {
        "id": "QJiI4tizHmxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 6 (3)"
      ],
      "metadata": {
        "id": "BuWkWpHYHqMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris dataset loaded...\")\n",
        "\n",
        "print(iris.keys())\n",
        "\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(iris.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3,p = 1 )\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"Correct predicition results\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "print(\"Worng predicition results\",(1-accuracy_score(y_test,y_pred))*100,\"%\")"
      ],
      "metadata": {
        "id": "He1eCVeaHsI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 6 (4)"
      ],
      "metadata": {
        "id": "WYHORVSKHwSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris dataset loaded...\")\n",
        "\n",
        "print(iris.keys())\n",
        "\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(iris.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"Correct predicition results\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "print(\"Worng predicition results\",(1-accuracy_score(y_test,y_pred))*100,\"%\")\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris dataset loaded...\")\n",
        "\n",
        "print(iris.keys())\n",
        "\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(iris.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3,p = 1 )\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"Correct predicition results\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "print(\"Worng predicition results\",(1-accuracy_score(y_test,y_pred))*100,\"%\")\n"
      ],
      "metadata": {
        "id": "wCn1d79dHyVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TW 6 (5)"
      ],
      "metadata": {
        "id": "6LovZVEIH1nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris dataset loaded...\")\n",
        "\n",
        "print(iris.keys())\n",
        "\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(iris.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3, metric = 'cosine')\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"Correct predicition results\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "print(\"Worng predicition results\",(1-accuracy_score(y_test,y_pred))*100,\"%\")\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris dataset loaded...\")\n",
        "\n",
        "print(iris.keys())\n",
        "\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\",str(iris.target_names[i]))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=3,p = 1 )\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "print(\"Confusion Matrix\")\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "\n",
        "print(\"Correct predicition results\",accuracy_score(y_test,y_pred)*100,\"%\")\n",
        "print(\"Worng predicition results\",(1-accuracy_score(y_test,y_pred))*100,\"%\")\n"
      ],
      "metadata": {
        "id": "sJFNODrZH3Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2iUqeZ2zH6Ax"
      }
    }
  ]
}